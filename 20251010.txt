üìò 1. Input Fidelity

Q: How is semantic ambiguity from email requests mitigated in the initial extraction step, especially when formats or sender templates differ?

	‚Ä¢	Do you rely on term libraries only?
	‚Ä¢	Is there semantic parsing beyond regex/configured terms?

‚∏ª

üß† 2. Model Logic: Calculation One

Q: What rules or inference methods are implemented in ‚ÄúCalculation One‚Äù?
Is this purely symbolic logic (e.g. contains/starts with), or do you leverage trained classifiers or embeddings for term disambiguation?

	‚Ä¢	What happens when multiple SWIFT codes are detected?
	‚Ä¢	Can you dynamically weigh likely candidates based on historical pairing (e.g., with BIC-IBAN mappings or country codes)?

‚∏ª

üë• 3. Manual Check

Q: What determines when a manual check is required? Is it confidence-based (e.g., threshold under 80%) or purely deterministic (e.g., field not found)?

	‚Ä¢	Can the system learn from overrides to reduce future human interventions?
	‚Ä¢	How is disagreement logged between model suggestion and human decision?

‚∏ª

üîÅ 4. Training & Feedback Loops

Q: When a correction is made in Manual Check, how is that fed back into the training pipeline for daily or weekly model improvements?

	‚Ä¢	Are you using annotation queues to retrain embeddings or prompt templates?
	‚Ä¢	How do you prevent overfitting on edge cases?

‚∏ª

üîê 5. Security & Isolation

Q: Is each Calculation Logic block sandboxed per customer/tenant, or are rules/models shared across environments?

	‚Ä¢	Is there tenancy-aware logic enforcement?
	‚Ä¢	What‚Äôs the fallback if one customer introduces conflicting overrides?

‚∏ª

üß± 6. Explainability & Audit

Q: Can every data element in the generated MT-103/202 be back-traced to the exact field/phrase/term in the originating document?

	‚Ä¢	How is provenance stored (e.g., hash, pointer, annotated file)?
	‚Ä¢	Can this be exported in PDF or XML for regulators?

‚∏ª

üîß 7. Agent Composition

Q: Can multiple agents be composed for compound extraction (e.g., one for payment type, another for bank metadata, another for compliance logic)?

	‚Ä¢	Can they pass context between each other?
	‚Ä¢	How do you prevent agents from conflicting when accessing overlapping fields (e.g. 56A and 57A)?

‚∏ª

üîÑ 8. Fallback & Failover

Q: In case of upstream system errors (e.g., COBAM unavailable), how does the agent reroute or gracefully degrade its logic?

	‚Ä¢	Is there a fallback policy?
	‚Ä¢	Can you defer certain fields to human queues and still process others autonomously?

‚∏ª

üß™ 9. Testing & Validation

Q: What is your test coverage model? How many synthetic permutations are needed to certify a new rule or scenario?

	‚Ä¢	Are tests deterministic or probabilistic?
	‚Ä¢	Do you support A/B logic flows for experimental rules?

‚∏ª

üìà 10. Performance & Governance

Q: What metrics are tracked to assess agent reliability, and how is drift or degradation in field-level accuracy detected?

	‚Ä¢	Field-level F1 scores?
	‚Ä¢	Time-to-resolution benchmarks?
	‚Ä¢	Volume of overridden outputs?

‚úÖ Expanded Strengths of DeepSee
	1.	Scenario-driven agent workflows
Pre-built modular workflows (e.g. for SSI, FX settlement, CMR ID validation) allow rapid prototyping and audit-ready agent execution paths.
	2.	Human-in-the-loop feedback loops
Manual validation steps are embedded directly into the process, supporting real-time supervision and progressive agent improvement.
	3.	Self-service term libraries
Business users can define and edit extraction rules and domain-specific terms (e.g. CMR ID, SWIFT, ABA) without engineering intervention.
	4.	Synthetic data support
Enables the use of synthetic and historical data to bootstrap agents where labeled data is scarce ‚Äî critical for edge cases and GRC scenarios.
	5.	Agent versioning & traceability
Maintains a lineage of agent actions, inputs, and rule evaluations ‚Äî ideal for regulatory evidence packs and model governance reviews.
	6.	Lightweight UI for SMEs
Unified document ingestion, annotation, and reconciliation UI makes the platform accessible for non-technical operations staff.
	7.	Multi-format input handling
Seamlessly ingests structured (Excel, CSV), semi-structured (PDF), and unstructured (email, text) documents in a common processing flow.
	8.	Process composability
Allows chaining of agent tasks (e.g. extract ‚Üí reconcile ‚Üí validate) to support increasingly complex operations.
	9.	Cross-system stitching
Ability to reconcile fields across multiple sources (email, COBAM, OPICS, 1SPI) supports multi-source validation and fraud prevention.

‚∏ª

‚ö†Ô∏è Expanded Limitations of DeepSee
	1.	Opaque LLM components
While agent flows are visible, the LLM decisioning behind some components (e.g. term identification, disambiguation) lacks full transparency.
	2.	Scalability limitations for training
The daily retraining cycles are operationally heavy and may not scale linearly across multiple business domains without cost increases.
	3.	Limited native connectors
Integrations to internal systems (e.g. EDIF, internal GRC tools) are not readily available and require custom development.
	4.	UI constraints for power users
The no-code interface, while SME-friendly, may frustrate advanced users needing scripting, bulk updates, or fine-grained control.
	5.	Cost overhead for scaling
Licensing and cloud usage costs may rise sharply with increased document volume and real-time agent requirements.
	6.	Rigid process templates
Preconfigured workflows can become a constraint when business logic evolves faster than the platform configuration options.
	7.	No full agent autonomy
Currently relies heavily on manual checkpoints; lacks advanced agent autonomy or auto-resolution capabilities (e.g. unsupervised retraining).
	8.	Monitoring dashboards are not customizable
Observability is limited to default KPIs; deeper telemetry integration or user-defined metrics is not yet available.
	9.	Complexity of deployment in air-gapped environments
If Wells Fargo has strict network segmentation or air-gapped systems, cloud-native deployment could be hindered.


‚úÖ Expanded Strengths of DeepSee
	1.	Scenario-driven agent workflows
Pre-built modular workflows (e.g. for SSI, FX settlement, CMR ID validation) allow rapid prototyping and audit-ready agent execution paths.
	2.	Human-in-the-loop feedback loops
Manual validation steps are embedded directly into the process, supporting real-time supervision and progressive agent improvement.
	3.	Self-service term libraries
Business users can define and edit extraction rules and domain-specific terms (e.g. CMR ID, SWIFT, ABA) without engineering intervention.
	4.	Synthetic data support
Enables the use of synthetic and historical data to bootstrap agents where labeled data is scarce ‚Äî critical for edge cases and GRC scenarios.
	5.	Agent versioning & traceability
Maintains a lineage of agent actions, inputs, and rule evaluations ‚Äî ideal for regulatory evidence packs and model governance reviews.
	6.	Lightweight UI for SMEs
Unified document ingestion, annotation, and reconciliation UI makes the platform accessible for non-technical operations staff.
	7.	Multi-format input handling
Seamlessly ingests structured (Excel, CSV), semi-structured (PDF), and unstructured (email, text) documents in a common processing flow.
	8.	Process composability
Allows chaining of agent tasks (e.g. extract ‚Üí reconcile ‚Üí validate) to support increasingly complex operations.
	9.	Cross-system stitching
Ability to reconcile fields across multiple sources (email, COBAM, OPICS, 1SPI) supports multi-source validation and fraud prevention.

‚∏ª

‚ö†Ô∏è Expanded Limitations of DeepSee
	1.	Opaque LLM components
While agent flows are visible, the LLM decisioning behind some components (e.g. term identification, disambiguation) lacks full transparency.
	2.	Scalability limitations for training
The daily retraining cycles are operationally heavy and may not scale linearly across multiple business domains without cost increases.
	3.	Limited native connectors
Integrations to internal systems (e.g. EDIF, internal GRC tools) are not readily available and require custom development.
	4.	UI constraints for power users
The no-code interface, while SME-friendly, may frustrate advanced users needing scripting, bulk updates, or fine-grained control.
	5.	Cost overhead for scaling
Licensing and cloud usage costs may rise sharply with increased document volume and real-time agent requirements.
	6.	Rigid process templates
Preconfigured workflows can become a constraint when business logic evolves faster than the platform configuration options.
	7.	No full agent autonomy
Currently relies heavily on manual checkpoints; lacks advanced agent autonomy or auto-resolution capabilities (e.g. unsupervised retraining).
	8.	Monitoring dashboards are not customizable
Observability is limited to default KPIs; deeper telemetry integration or user-defined metrics is not yet available.
	9.	Complexity of deployment in air-gapped environments
If Wells Fargo has strict network segmentation or air-gapped systems, cloud-native deployment could be hindered.

