ğŸ“˜ 1. Input Fidelity

Q: How is semantic ambiguity from email requests mitigated in the initial extraction step, especially when formats or sender templates differ?

	â€¢	Do you rely on term libraries only?
	â€¢	Is there semantic parsing beyond regex/configured terms?

â¸»

ğŸ§  2. Model Logic: Calculation One

Q: What rules or inference methods are implemented in â€œCalculation Oneâ€?
Is this purely symbolic logic (e.g. contains/starts with), or do you leverage trained classifiers or embeddings for term disambiguation?

	â€¢	What happens when multiple SWIFT codes are detected?
	â€¢	Can you dynamically weigh likely candidates based on historical pairing (e.g., with BIC-IBAN mappings or country codes)?

â¸»

ğŸ‘¥ 3. Manual Check

Q: What determines when a manual check is required? Is it confidence-based (e.g., threshold under 80%) or purely deterministic (e.g., field not found)?

	â€¢	Can the system learn from overrides to reduce future human interventions?
	â€¢	How is disagreement logged between model suggestion and human decision?

â¸»

ğŸ” 4. Training & Feedback Loops

Q: When a correction is made in Manual Check, how is that fed back into the training pipeline for daily or weekly model improvements?

	â€¢	Are you using annotation queues to retrain embeddings or prompt templates?
	â€¢	How do you prevent overfitting on edge cases?

â¸»

ğŸ” 5. Security & Isolation

Q: Is each Calculation Logic block sandboxed per customer/tenant, or are rules/models shared across environments?

	â€¢	Is there tenancy-aware logic enforcement?
	â€¢	Whatâ€™s the fallback if one customer introduces conflicting overrides?

â¸»

ğŸ§± 6. Explainability & Audit

Q: Can every data element in the generated MT-103/202 be back-traced to the exact field/phrase/term in the originating document?

	â€¢	How is provenance stored (e.g., hash, pointer, annotated file)?
	â€¢	Can this be exported in PDF or XML for regulators?

â¸»

ğŸ”§ 7. Agent Composition

Q: Can multiple agents be composed for compound extraction (e.g., one for payment type, another for bank metadata, another for compliance logic)?

	â€¢	Can they pass context between each other?
	â€¢	How do you prevent agents from conflicting when accessing overlapping fields (e.g. 56A and 57A)?

â¸»

ğŸ”„ 8. Fallback & Failover

Q: In case of upstream system errors (e.g., COBAM unavailable), how does the agent reroute or gracefully degrade its logic?

	â€¢	Is there a fallback policy?
	â€¢	Can you defer certain fields to human queues and still process others autonomously?

â¸»

ğŸ§ª 9. Testing & Validation

Q: What is your test coverage model? How many synthetic permutations are needed to certify a new rule or scenario?

	â€¢	Are tests deterministic or probabilistic?
	â€¢	Do you support A/B logic flows for experimental rules?

â¸»

ğŸ“ˆ 10. Performance & Governance

Q: What metrics are tracked to assess agent reliability, and how is drift or degradation in field-level accuracy detected?

	â€¢	Field-level F1 scores?
	â€¢	Time-to-resolution benchmarks?
	â€¢	Volume of overridden outputs?